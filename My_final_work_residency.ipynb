{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Import some packages\n\n* Numpy\n* TensorFlow / Keras\n* Sklearn\n* Os\n* Pandas\n* Zipfile\n* OpenCV\n* Matplotlib\n* Time\n* Mat73\n* Tqdm\n* Gc\n* Classifiers\n* Among others","metadata":{}},{"cell_type":"code","source":"#Install packages before import\n\n!pip install mat73\n!pip install git+https://github.com/qubvel/classification_models.git\n!pip install -U segmentation-models\n!pip install -U albumentations\n!pip install openpyxl","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:15:43.963495Z","iopub.execute_input":"2022-03-08T21:15:43.964111Z","iopub.status.idle":"2022-03-08T21:16:27.454674Z","shell.execute_reply.started":"2022-03-08T21:15:43.964020Z","shell.execute_reply":"2022-03-08T21:16:27.453794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing packages\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nimport os\nimport pandas as pd\nimport zipfile\nimport cv2\nimport matplotlib.pyplot as plt\nimport datetime\nimport mat73\nfrom tqdm import tqdm\nimport gc\nfrom classification_models.tfkeras import Classifiers\nimport random\nimport scipy.stats as stats\nfrom scipy.stats import shapiro\nimport seaborn as sns\nfrom scipy.stats import f_oneway\nfrom statsmodels.stats.multicomp import MultiComparison\nimport albumentations as A\n%env SM_FRAMEWORK=tf.keras\nimport segmentation_models as sm\nsm.set_framework('tf.keras')","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:16:27.457309Z","iopub.execute_input":"2022-03-08T21:16:27.457600Z","iopub.status.idle":"2022-03-08T21:16:33.860192Z","shell.execute_reply.started":"2022-03-08T21:16:27.457561Z","shell.execute_reply":"2022-03-08T21:16:33.859409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Loading data and preprocessing a little bit\nBrain tumor data","metadata":{}},{"cell_type":"code","source":"# Access the path, load and preprocessing the informations\n\nimages_dir_1 = os.path.join('../input/brain-tumor-dataset-original/brainTumorDataPublic_1-766/')\nimages_dir_2 = os.path.join('../input/brain-tumor-dataset-original/brainTumorDataPublic_1533-2298/')\nimages_dir_3 = os.path.join('../input/brain-tumor-dataset-original/brainTumorDataPublic_2299-3064/')\nimages_dir_4 = os.path.join('../input/brain-tumor-dataset-original/brainTumorDataPublic_767-1532/')\n\nimages_list_1 = os.listdir(images_dir_1)\nimages_list_2 = os.listdir(images_dir_2)\nimages_list_3 = os.listdir(images_dir_3)\nimages_list_4 = os.listdir(images_dir_4)\n\nimages_dir = ['../input/brain-tumor-dataset-original/brainTumorDataPublic_1-766/', \n              '../input/brain-tumor-dataset-original/brainTumorDataPublic_1533-2298/', \n              '../input/brain-tumor-dataset-original/brainTumorDataPublic_2299-3064/', \n              '../input/brain-tumor-dataset-original/brainTumorDataPublic_767-1532/']\nimages_list = [images_list_1, images_list_2, images_list_3, images_list_4]\n\nimages = []\nmasks = []\nlabels = []\n\nfor i,j in zip(images_dir, images_list):\n    for k in tqdm(j):\n        path_image = str(i) + str(k)\n        data_dict = mat73.loadmat(path_image)\n        images.append((data_dict['cjdata']['image'])) #This is a problematic part\n        masks.append((data_dict['cjdata']['tumorMask']*1).astype('uint8'))\n        labels = np.append(labels, data_dict['cjdata']['label'])\nprint('There are {} images.'.format(len(images)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing the label\n\nle = LabelEncoder()\nlabels_labelencoder = pd.DataFrame(np.array(labels).T).apply(le.fit_transform)\nenc = OneHotEncoder()\nenc.fit(labels_labelencoder)\nlabels_labelencoder_onehotencoder = enc.transform(labels_labelencoder).toarray()\nlabels_labelencoder_onehotencoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#What is data_dict?\n\ndata_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deleting useless data\n\ndel data_dict\ndel labels\ndel labels_labelencoder\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Spliting/preprocessing and saving the data into train/validation and test sets","metadata":{}},{"cell_type":"markdown","source":"**Classification**","metadata":{}},{"cell_type":"code","source":"#Spliting\n\nX_train_validation, X_test, y_train_validation, y_test = train_test_split(images, labels_labelencoder_onehotencoder, test_size=0.2, random_state=0)\n\n# Deleting useless data\n\ndel images\ndel labels_labelencoder_onehotencoder\ngc.collect()\n\n#Saving\n\nnp.save('X_train_validation.npy', X_train_validation)\nnp.save('X_test.npy', X_test)\nnp.save('y_train_validation.npy',y_train_validation)\nnp.save('y_test.npy', y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Segmentation**","metadata":{}},{"cell_type":"code","source":"#Spliting\n\nX_train_validation, X_test, y_train_validation, y_test = train_test_split(images, masks, test_size=0.2, random_state=0)\n\n# Deleting useless data\n\ndel images\ndel masks\ngc.collect()\n\n#Saving\n\nnp.save('X_train_validation.npy', X_train_validation)\nnp.save('X_test.npy', X_test)\nnp.save('y_train_validation.npy',y_train_validation)\nnp.save('y_test.npy', y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show images\n\nf, axarr = plt.subplots(2,2)\naxarr[0,0].imshow(X_train_validation[6], cmap='gray')\naxarr[0,1].imshow(X_train_validation[7], cmap='gray')\naxarr[1,0].imshow(y_train_validation[6], cmap='gray')\naxarr[1,1].imshow(y_train_validation[7], cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Initializing brain tumor classification","metadata":{}},{"cell_type":"code","source":"# Loading the saved data for classification - PNG method\n\nos.makedirs(\"./X_train_validation_png\")\nos.makedirs(\"./X_test_png\")\n\nX_train_validation_jpg = np.load('../input/brain-tumor-dataset-modified/X_train_validation.npy', allow_pickle = True)\nfor i in tqdm(range(0,2451)):    \n    X1 = cv2.convertScaleAbs(X_train_validation_jpg[i], alpha = 0.03).tolist()\n    plt.imsave('./X_train_validation_png/image0' + str(i) + '.png', X1, cmap='gray')\n    \n\nX_test_jpg = np.load('../input/brain-tumor-dataset-modified/X_test.npy', allow_pickle = True)\nfor i in tqdm(range(0,613)):    \n    X2 = cv2.convertScaleAbs(X_test_jpg[i], alpha = 0.03).tolist()\n    plt.imsave('./X_test_png/image0' + str(i) + '.png', X2, cmap='gray')\n    \ny_train_validation = np.load('../input/brain-tumor-dataset-modified/y_train_validation.npy', allow_pickle = True)\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save zipfile of X_train_validation\n\nimport os\nimport zipfile\n \nfantasy_zip = zipfile.ZipFile('./X_train_validation_png.zip', 'w')\n \nfor folder, subfolders, files in os.walk('./X_train_validation_png'):\n \n    for file in files:\n        if file.endswith('.png'):\n            fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), './PNEUMOTHORAX'), compress_type = zipfile.ZIP_DEFLATED)\n \nfantasy_zip.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save zipfile of X_test\n\nimport os\nimport zipfile\n \nfantasy_zip = zipfile.ZipFile('./X_test_png.zip', 'w')\n \nfor folder, subfolders, files in os.walk('./X_test_png'):\n \n    for file in files:\n        if file.endswith('.png'):\n            fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), './PNEUMOTHORAX'), compress_type = zipfile.ZIP_DEFLATED)\n \nfantasy_zip.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good point to restart","metadata":{}},{"cell_type":"code","source":"# Read image and resize using INTER_AREA\n\nx_train_validation_data = list()\nx_test_data = list()\n\nfor number in tqdm(range(0,2451)):\n    im = cv2.imread('../input/images-grayscale/X_train_validation_png/image0' + str(number) + '.png',cv2.IMREAD_GRAYSCALE)\n    im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n    x_train_validation_data.append(im)\nfor number in tqdm(range(0,613)):\n    im = cv2.imread('../input/images-grayscale/X_test_png/image0' + str(number) + '.png',cv2.IMREAD_GRAYSCALE)\n    im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n    x_test_data.append(im)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the y data\ny_train_validation = np.load('../input/brain-tumor-dataset-modified/y_train_validation.npy', allow_pickle = True)\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show images\n\nf, axarr = plt.subplots(2,2)\naxarr[0,0].imshow(x_train_validation_data[2441], cmap='gray')\naxarr[0,1].imshow(x_train_validation_data[2449], cmap='gray')\naxarr[1,0].imshow(x_train_validation_data[6], cmap='gray')\naxarr[1,1].imshow(x_train_validation_data[7], cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reshape the images\n\nx_train_validation_data = (np.array(x_train_validation_data).reshape((len(x_train_validation_data), 256, 256, 1))).astype('float16') / 255\nx_test_data = (np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1))).astype('float16') / 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Activating the TPU\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classifiers avaliable in the package\n\nClassifiers.models_names()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\ndef custom_f1(y_true, y_pred):    \n    def recall_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        \n        recall = TP / (Positives+K.epsilon())    \n        return recall \n    \n    \n    def precision_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    \n        precision = TP / (Pred_Positives+K.epsilon())\n        return precision \n    \n    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n    \n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T01:39:00.166874Z","iopub.execute_input":"2022-02-24T01:39:00.167134Z","iopub.status.idle":"2022-02-24T01:39:00.175291Z","shell.execute_reply.started":"2022-02-24T01:39:00.167106Z","shell.execute_reply":"2022-02-24T01:39:00.174332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constructing the NN and executing the tran and validation\nstart_time = datetime.datetime.now()\n\nkfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\nresultados = []\nj = 0\n\nfpr = [0] * 10\nfor i in range(10):\n    fpr[i] = [0] * 3\n    \ntpr = [0] * 10\nfor i in range(10):\n    tpr[i] = [0] * 3\n\nfor train_index, test_index in kfold.split(x_train_validation_data,np.zeros(shape = (x_train_validation_data.shape[0],1))):\n    \n    SIZE = len(x_train_validation_data[0])\n\n    with tpu_strategy.scope():\n\n        seresnext50, _ = Classifiers.get('mobilenet')\n\n        input_tensor = keras.layers.Input(shape=(SIZE, SIZE,1))\n        x = keras.layers.Conv2D(3,(1,1),padding='same')(input_tensor)\n        base_model = seresnext50(input_shape = (SIZE, SIZE,3), include_top = False, weights = 'imagenet')(x)\n        y = keras.layers.GlobalAveragePooling2D()(base_model)\n        y2 = tf.keras.layers.BatchNormalization()(y)\n        y3 = tf.keras.layers.Flatten()(y2)\n        y4 = tf.keras.layers.Dense(1026, activation=\"relu\")(y3)\n        y5 = tf.keras.layers.Dropout(0.5)(y4)\n        output = keras.layers.Dense(3, activation='softmax')(y5)\n        model = keras.models.Model(inputs=[input_tensor], outputs=[output])\n        model.summary()\n\n        model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[tf.keras.metrics.AUC(multi_label=True), custom_f1,'accuracy',\n                                                                                 tf.keras.metrics.Precision(), keras.metrics.Recall(),tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n\n        if j == 0:\n            k = 'val_custom_f1'\n        else:\n            k = 'val_custom_f1'#_'+str(j)\n\n        name= 'MRI_mobilenet.h5'\n        rlr = ReduceLROnPlateau(monitor = k, factor = 0.1, patience = 3, verbose = 1, min_delta = 1e-4, min_lr = 1e-6, mode = 'max', cooldown=1)\n        ckp = ModelCheckpoint(name,monitor = k,verbose = 1, save_best_only = True, mode = 'max')    \n        es = EarlyStopping(monitor = k, min_delta = 1e-4, patience =10, mode = 'max', restore_best_weights = True, verbose = 1)\n        history = model.fit(x_train_validation_data[train_index], y_train_validation[train_index],\n                            batch_size = 64, epochs = 100, \n                            validation_data = (x_train_validation_data[test_index], y_train_validation[test_index]),\n                            callbacks=[rlr,es,ckp])\n        \n        dependencies = {'custom_f1': custom_f1}\n        model_= tf.keras.models.load_model('./MRI_mobilenet.h5', custom_objects=dependencies)\n        model_.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[tf.keras.metrics.AUC(multi_label=True), custom_f1,'accuracy',\n                                                                          tf.keras.metrics.Precision(), keras.metrics.Recall(),tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n        precision = model_.evaluate(x_train_validation_data[test_index],\n                                    y_train_validation[test_index],\n                                    batch_size = 64)\n        \n        \n        resultados.append(precision)\n        j += 1\n        \nend_time = datetime.datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here you can start by now","metadata":{}},{"cell_type":"code","source":"alpha = 0.05\nmean_class = pd.read_csv('../input/mean-class-ok/Mdias_Class.csv')\nmean_class","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AUC\n\n#Variance Homogeneity       igualdade das var > 0.05\nprint(stats.levene(mean_class['AUC_Seresnext50'],mean_class['AUC_Vgg16'],mean_class['AUC_Mobilenet'], center = 'median'))\n\n#Normality                  normalidade da distr. >0.05\nprint(shapiro(mean_class['AUC_Seresnext50']),shapiro(mean_class['AUC_Vgg16']),shapiro(mean_class['AUC_Mobilenet']))\nsns.displot(mean_class['AUC_Seresnext50'], kind = 'kde');\nsns.displot(mean_class['AUC_Vgg16'], kind = 'kde');\nsns.displot(mean_class['AUC_Mobilenet'], kind = 'kde');\n\n# <4 ANOVA anyway\n# >4 Kruskal-Wallis Test\nprint(np.var(mean_class['AUC_Seresnext50']), np.var(mean_class['AUC_Vgg16']), np.var(mean_class['AUC_Mobilenet']))\nprint(3.1033555129181136e-06/4.332674250218629e-07)\n\n#ANOVA\n#_,p = f_oneway(mean_class['AUC_Seresnext50'],mean_class['AUC_Vgg16'],mean_class['AUC_Mobilenet'])\n#Kruskal-Wallis    igual > 0.05\n_,p = stats.kruskal(mean_class['AUC_Seresnext50'],mean_class['AUC_Vgg16'],mean_class['AUC_Mobilenet'])\nprint(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#F1\n\n#Variance Homogeneity       igualdade das var > 0.05\nprint(stats.levene(mean_class['F1_Seresnext50'],mean_class['F1_Vgg16'],mean_class['F1_Mobilenet'], center = 'median'))\n\n#Normality                  normalidade da distr. >0.05\nprint(shapiro(mean_class['F1_Seresnext50']),shapiro(mean_class['F1_Vgg16']),shapiro(mean_class['F1_Mobilenet']))\nsns.displot(mean_class['F1_Seresnext50'], kind = 'kde');\nsns.displot(mean_class['F1_Vgg16'], kind = 'kde');\nsns.displot(mean_class['F1_Mobilenet'], kind = 'kde');\n\n# <4 ANOVA anyway\n# >4 Kruskal-Wallis Test\nprint(np.var(mean_class['F1_Seresnext50']), np.var(mean_class['F1_Vgg16']), np.var(mean_class['F1_Mobilenet']))\nprint(4.2278554688608024e-05/3.966609370570272e-06)\n\n#ANOVA\n#_,p = f_oneway(mean_class['F1_Seresnext50'],mean_class['F1_Vgg16'],mean_class['F1_Mobilenet'])\n#Kruskal-Wallis    igual > 0.05\n_,p = stats.kruskal(mean_class['F1_Seresnext50'],mean_class['F1_Vgg16'],mean_class['F1_Mobilenet'])\nprint(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Acc\n\n#Variance Homogeneity       igualdade das var > 0.05\nprint(stats.levene(mean_class['Acc_Seresnext50'],mean_class['Acc_Vgg16'],mean_class['Acc_Mobilenet'], center = 'median'))\n\n#Normality                  normalidade da distr. >0.05\nprint(shapiro(mean_class['Acc_Seresnext50']),shapiro(mean_class['Acc_Vgg16']),shapiro(mean_class['Acc_Mobilenet']))\nsns.displot(mean_class['Acc_Seresnext50'], kind = 'kde');\nsns.displot(mean_class['Acc_Vgg16'], kind = 'kde');\nsns.displot(mean_class['Acc_Mobilenet'], kind = 'kde');\n\n# <4 ANOVA anyway\n# >4 Kruskal-Wallis Test\nprint(np.var(mean_class['Acc_Seresnext50']), np.var(mean_class['Acc_Vgg16']), np.var(mean_class['Acc_Mobilenet']))\nprint(4.606542803178421e-05/3.7360412156123934e-06)\n\n#ANOVA\n#_,p = f_oneway(mean_class['Acc_Seresnext50'],mean_class['Acc_Vgg16'],mean_class['Acc_Mobilenet'])\n#Kruskal-Wallis    igual > 0.05\n_,p = stats.kruskal(mean_class['Acc_Seresnext50'],mean_class['Acc_Vgg16'],mean_class['Acc_Mobilenet'])\nprint(p)\n#Há deferença\np1 = stats.kruskal(mean_class['Acc_Seresnext50'],mean_class['Acc_Vgg16'])\nprint(p1)\np2 = stats.kruskal(mean_class['Acc_Seresnext50'],mean_class['Acc_Mobilenet'])\nprint(p2)\np3 = stats.kruskal(mean_class['Acc_Vgg16'],mean_class['Acc_Mobilenet'])\nprint(p3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prec\n\n#Variance Homogeneity       igualdade das var > 0.05\nprint(stats.levene(mean_class['Prec_Seresnext50'],mean_class['Prec_Vgg16'],mean_class['Prec_Mobilenet'], center = 'median'))\n\n#Normality                  normalidade da distr. >0.05\nprint(shapiro(mean_class['Prec_Seresnext50']),shapiro(mean_class['Prec_Vgg16']),shapiro(mean_class['Prec_Mobilenet']))\nsns.displot(mean_class['Prec_Seresnext50'], kind = 'kde');\nsns.displot(mean_class['Prec_Vgg16'], kind = 'kde');\nsns.displot(mean_class['Prec_Mobilenet'], kind = 'kde');\n\n# <4 ANOVA anyway\n# >4 Kruskal-Wallis Test\nprint(np.var(mean_class['Prec_Seresnext50']), np.var(mean_class['Prec_Vgg16']), np.var(mean_class['Prec_Mobilenet']))\nprint(2.5833736912516415e-05/4.0185712494159865e-06)\n\n#ANOVA igual > 0.05\n_,p = f_oneway(mean_class['Prec_Seresnext50'],mean_class['Prec_Vgg16'],mean_class['Prec_Mobilenet'])\n\n#Kruskal-Wallis    igual > 0.05\n#_,p = stats.kruskal(mean_class['Prec_Seresnext50'],mean_class['Prec_Vgg16'],mean_class['Prec_Mobilenet'])\nprint(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Rec\n\n#Variance Homogeneity       igualdade das var > 0.05\nprint(stats.levene(mean_class['Rec_Seresnext50'],mean_class['Rec_Vgg16'],mean_class['Rec_Mobilenet'], center = 'median'))\n\n#Normality                  normalidade da distr. >0.05\nprint(shapiro(mean_class['Rec_Seresnext50']),shapiro(mean_class['Rec_Vgg16']),shapiro(mean_class['Rec_Mobilenet']))\nsns.displot(mean_class['Rec_Seresnext50'], kind = 'kde');\nsns.displot(mean_class['Rec_Vgg16'], kind = 'kde');\nsns.displot(mean_class['Rec_Mobilenet'], kind = 'kde');\n\n# <4 ANOVA anyway\n# >4 Kruskal-Wallis Test\nprint(np.var(mean_class['Rec_Seresnext50']), np.var(mean_class['Rec_Vgg16']), np.var(mean_class['Rec_Mobilenet']))\nprint(6.229484832056975e-05/3.99211178658067e-06)\n\n#ANOVA\n#_,p = f_oneway(mean_class['Rec_Seresnext50'],mean_class['Rec_Vgg16'],mean_class['Rec_Mobilenet'])\n#Kruskal-Wallis    igual > 0.05\n_,p = stats.kruskal(mean_class['Rec_Seresnext50'],mean_class['Rec_Vgg16'],mean_class['Rec_Mobilenet'])\nprint(p)\n#Há deferença\np1 = stats.kruskal(mean_class['Rec_Seresnext50'],mean_class['Rec_Vgg16'])\nprint(p1)\np2 = stats.kruskal(mean_class['Rec_Seresnext50'],mean_class['Rec_Mobilenet'])\nprint(p2)\np3 = stats.kruskal(mean_class['Rec_Vgg16'],mean_class['Rec_Mobilenet'])\nprint(p3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Variance Homogeneity\n\n#AUC\nstats.levene(mean_class['AUC_Seresnext50'],mean_class['AUC_Vgg16'],mean_class['AUC_Mobilenet'], center = 'median')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#F1\nstats.levene(mean_class['F1_Seresnext50'],mean_class['F1_Vgg16'],mean_class['F1_Mobilenet'], center = 'median')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Acc\nstats.levene(mean_class['Acc_Seresnext50'],mean_class['Acc_Vgg16'],mean_class['Acc_Mobilenet'], center = 'median')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Precision\nstats.levene(mean_class['Prec_Seresnext50'],mean_class['Prec_Vgg16'],mean_class['Prec_Mobilenet'], center = 'median')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Recall\nstats.levene(mean_class['Rec_Seresnext50'],mean_class['Rec_Vgg16'],mean_class['Rec_Mobilenet'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normality\n\n#AUC\nshapiro(mean_class['AUC_Seresnext50']),shapiro(mean_class['AUC_Vgg16']),shapiro(mean_class['AUC_Mobilenet'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(mean_class['AUC_Vgg16'], kind = 'kde');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#F1\nshapiro(mean_class['F1_Seresnext50']),shapiro(mean_class['F1_Vgg16']),shapiro(mean_class['F1_Mobilenet'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Acc\nshapiro(mean_class['Acc_Seresnext50']),shapiro(mean_class['Acc_Vgg16']),shapiro(mean_class['Acc_Mobilenet'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(mean_class['Acc_Seresnext50'], kind = 'kde');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Precision\nshapiro(mean_class['Prec_Seresnext50']),shapiro(mean_class['Prec_Vgg16']),shapiro(mean_class['Prec_Mobilenet'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Recall\nshapiro(mean_class['Rec_Seresnext50']),shapiro(mean_class['Rec_Vgg16']),shapiro(mean_class['Rec_Mobilenet'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_,p = f_oneway(mean_class['AUC_Seresnext50'],mean_class['AUC_Vgg16'],mean_class['AUC_Mobilenet'])\np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_,p = f_oneway(mean_class['F1_Seresnext50'],mean_class['F1_Vgg16'],mean_class['F1_Mobilenet'])\np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_,p = f_oneway(mean_class['Acc_Seresnext50'],mean_class['Acc_Vgg16'],mean_class['Acc_Mobilenet'])\np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_,p = f_oneway(mean_class['Prec_Seresnext50'],mean_class['Prec_Vgg16'],mean_class['Prec_Mobilenet'])\np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_,p = f_oneway(mean_class['Rec_Seresnext50'],mean_class['Rec_Vgg16'],mean_class['Rec_Mobilenet'])\np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultado_algorithm_AUC = {'AUC': np.concatenate([mean_class['AUC_Seresnext50'], mean_class['AUC_Vgg16'], mean_class['AUC_Mobilenet']]),\n                         'algorithm': ['AUC_Seresnext50','AUC_Seresnext50','AUC_Seresnext50','AUC_Seresnext50','AUC_Seresnext50','AUC_Seresnext50','AUC_Seresnext50','AUC_Seresnext50','AUC_Seresnext50','AUC_Seresnext50',\n                                      'AUC_Vgg16','AUC_Vgg16','AUC_Vgg16','AUC_Vgg16','AUC_Vgg16','AUC_Vgg16','AUC_Vgg16','AUC_Vgg16','AUC_Vgg16','AUC_Vgg16',\n                                      'AUC_Mobilenet','AUC_Mobilenet','AUC_Mobilenet','AUC_Mobilenet','AUC_Mobilenet','AUC_Mobilenet','AUC_Mobilenet','AUC_Mobilenet','AUC_Mobilenet','AUC_Mobilenet']}\nresults = pd.DataFrame(resultado_algorithm_AUC)\ncompara_algoritmos = MultiComparison(results['AUC'],results['algorithm'])\nteste_estatistico = compara_algoritmos.tukeyhsd()\nprint(teste_estatistico)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultado_algorithm_F1 = {'F1': np.concatenate([mean_class['F1_Seresnext50'], mean_class['F1_Vgg16'], mean_class['F1_Mobilenet']]),\n                         'algorithm': ['F1_Seresnext50','F1_Seresnext50','F1_Seresnext50','F1_Seresnext50','F1_Seresnext50','F1_Seresnext50','F1_Seresnext50','F1_Seresnext50','F1_Seresnext50','F1_Seresnext50',\n                                      'F1_Vgg16','F1_Vgg16','F1_Vgg16','F1_Vgg16','F1_Vgg16','F1_Vgg16','F1_Vgg16','F1_Vgg16','F1_Vgg16','F1_Vgg16',\n                                      'F1_Mobilenet','F1_Mobilenet','F1_Mobilenet','F1_Mobilenet','F1_Mobilenet','F1_Mobilenet','F1_Mobilenet','F1_Mobilenet','F1_Mobilenet','F1_Mobilenet']}\nresults = pd.DataFrame(resultado_algorithm_F1)\ncompara_algoritmos = MultiComparison(results['F1'],results['algorithm'])\nteste_estatistico = compara_algoritmos.tukeyhsd()\nprint(teste_estatistico)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultado_algorithm_Acc = {'Acc': np.concatenate([mean_class['Acc_Seresnext50'], mean_class['Acc_Vgg16'], mean_class['Acc_Mobilenet']]),\n                         'algorithm': ['Acc_Seresnext50','Acc_Seresnext50','Acc_Seresnext50','Acc_Seresnext50','Acc_Seresnext50','Acc_Seresnext50','Acc_Seresnext50','Acc_Seresnext50','Acc_Seresnext50','Acc_Seresnext50',\n                                      'Acc_Vgg16','Acc_Vgg16','Acc_Vgg16','Acc_Vgg16','Acc_Vgg16','Acc_Vgg16','Acc_Vgg16','Acc_Vgg16','Acc_Vgg16','Acc_Vgg16',\n                                      'Acc_Mobilenet','Acc_Mobilenet','Acc_Mobilenet','Acc_Mobilenet','Acc_Mobilenet','Acc_Mobilenet','Acc_Mobilenet','Acc_Mobilenet','Acc_Mobilenet','Acc_Mobilenet']}\nresults = pd.DataFrame(resultado_algorithm_Acc)\ncompara_algoritmos = MultiComparison(results['Acc'],results['algorithm'])\nteste_estatistico = compara_algoritmos.tukeyhsd()\nprint(teste_estatistico)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultado_algorithm_Prec = {'Prec': np.concatenate([mean_class['Prec_Seresnext50'], mean_class['Prec_Vgg16'], mean_class['Prec_Mobilenet']]),\n                         'algorithm': ['Prec_Seresnext50','Prec_Seresnext50','Prec_Seresnext50','Prec_Seresnext50','Prec_Seresnext50','Prec_Seresnext50','Prec_Seresnext50','Prec_Seresnext50','Prec_Seresnext50','Prec_Seresnext50',\n                                      'Prec_Vgg16','Prec_Vgg16','Prec_Vgg16','Prec_Vgg16','Prec_Vgg16','Prec_Vgg16','Prec_Vgg16','Prec_Vgg16','Prec_Vgg16','Prec_Vgg16',\n                                      'Prec_Mobilenet','Prec_Mobilenet','Prec_Mobilenet','Prec_Mobilenet','Prec_Mobilenet','Prec_Mobilenet','Prec_Mobilenet','Prec_Mobilenet','Prec_Mobilenet','Prec_Mobilenet']}\nresults = pd.DataFrame(resultado_algorithm_Prec)\ncompara_algoritmos = MultiComparison(results['Prec'],results['algorithm'])\nteste_estatistico = compara_algoritmos.tukeyhsd()\nprint(teste_estatistico)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultado_algorithm_Rec = {'Rec': np.concatenate([mean_class['Rec_Seresnext50'], mean_class['Rec_Vgg16'], mean_class['Rec_Mobilenet']]),\n                         'algorithm': ['Rec_Seresnext50','Rec_Seresnext50','Rec_Seresnext50','Rec_Seresnext50','Rec_Seresnext50','Rec_Seresnext50','Rec_Seresnext50','Rec_Seresnext50','Rec_Seresnext50','Rec_Seresnext50',\n                                      'Rec_Vgg16','Rec_Vgg16','Rec_Vgg16','Rec_Vgg16','Rec_Vgg16','Rec_Vgg16','Rec_Vgg16','Rec_Vgg16','Rec_Vgg16','Rec_Vgg16',\n                                      'Rec_Mobilenet','Rec_Mobilenet','Rec_Mobilenet','Rec_Mobilenet','Rec_Mobilenet','Rec_Mobilenet','Rec_Mobilenet','Rec_Mobilenet','Rec_Mobilenet','Rec_Mobilenet']}\nresults = pd.DataFrame(resultado_algorithm_Rec)\ncompara_algoritmos = MultiComparison(results['Rec'],results['algorithm'])\nteste_estatistico = compara_algoritmos.tukeyhsd()\nprint(teste_estatistico)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model - test\ndependencies = {'custom_f1': custom_f1}\nmodel_= tf.keras.models.load_model('./MRI_mobilenet.h5', custom_objects=dependencies)\nmodel_.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[tf.keras.metrics.AUC(multi_label=True), custom_f1,'accuracy',\n                                                                          tf.keras.metrics.Precision(), keras.metrics.Recall(),tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\nprecision = model_.evaluate(x_test_data,y_test,batch_size = 128)\nprint('Our F1-score score is {} - Test group.'.format(precision[2]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Complete Training","metadata":{}},{"cell_type":"code","source":"# Read image and resize using INTER_AREA\n\nx_train_validation_data = list()\nx_test_data = list()\n\nfor number in tqdm(range(0,2451)):\n    im = cv2.imread('../input/images-grayscale/X_train_validation_png/image0' + str(number) + '.png',cv2.IMREAD_GRAYSCALE)\n    im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n    x_train_validation_data.append(im)\n    \n#Loading the y data\ny_train_validation = np.load('../input/brain-tumor-dataset-modified/y_train_validation.npy', allow_pickle = True)\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Activating the TPU\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for number in tqdm(range(0,613)):\n    im = cv2.imread('../input/images-grayscale/X_test_png/image0' + str(number) + '.png',cv2.IMREAD_GRAYSCALE)\n    im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n    #_, im = FFT_log(im)\n    #_, im = NEX(im,0.41)\n    #_, im = cut_sup_inf(im, 0.9)\n    #_, im = low_pass(im, 50)\n    #_, im = noise(im, 1)\n    x_test_data.append(im)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\ndef custom_f1(y_true, y_pred):    \n    def recall_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        \n        recall = TP / (Positives+K.epsilon())    \n        return recall \n    \n    \n    def precision_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    \n        precision = TP / (Pred_Positives+K.epsilon())\n        return precision \n    \n    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n    \n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reshape the images\n\nx_train_validation_data = (np.array(x_train_validation_data).reshape((len(x_train_validation_data), 256, 256, 1))).astype('float16') / 255\nx_test_data = (np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1))).astype('float16') / 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = datetime.datetime.now()\nresultados = []\n\n\nSIZE = len(x_train_validation_data[0])\nseresnext50, _ = Classifiers.get('mobilenet')\n\ninput_tensor = keras.layers.Input(shape=(SIZE, SIZE,1))\nx = keras.layers.Conv2D(3,(1,1),padding='same')(input_tensor)\nbase_model = seresnext50(input_shape = (SIZE, SIZE,3), include_top = False, weights = 'imagenet')(x)\ny = keras.layers.GlobalAveragePooling2D()(base_model)\ny2 = tf.keras.layers.BatchNormalization()(y)\ny3 = tf.keras.layers.Flatten()(y2)\ny4 = tf.keras.layers.Dense(1026, activation=\"relu\")(y3)\ny5 = tf.keras.layers.Dropout(0.5)(y4)\noutput = keras.layers.Dense(3, activation='softmax')(y5)\nmodel = keras.models.Model(inputs=[input_tensor], outputs=[output])\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[tf.keras.metrics.AUC(multi_label=True), custom_f1,'accuracy',\n                                                                         tf.keras.metrics.Precision(), keras.metrics.Recall(),tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n\n\nname= 'MRI_mobilenet.h5'\nrlr = ReduceLROnPlateau(monitor = 'val_custom_f1', factor = 0.1, patience = 3, verbose = 1, min_delta = 1e-4, min_lr = 1e-6, mode = 'max', cooldown=1)\nckp = ModelCheckpoint(name,monitor = 'val_custom_f1', verbose = 1, save_best_only = True, mode = 'max')    \nes = EarlyStopping(monitor = 'val_custom_f1', min_delta = 1e-4, patience =10, mode = 'max', restore_best_weights = True, verbose = 1)\nhistory = model.fit(x_train_validation_data, y_train_validation,\n                    batch_size = 32, epochs = 100,\n                    validation_data = (x_test_data, y_test),\n                    callbacks=[rlr,es,ckp])\n\nend_time = datetime.datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dependencies = {'custom_f1': custom_f1}\nmodel_= tf.keras.models.load_model('./MRI_mobilenet.h5', custom_objects=dependencies)\nmodel_.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[tf.keras.metrics.AUC(multi_label=True), custom_f1,'accuracy',\n                                                                          tf.keras.metrics.Precision(), keras.metrics.Recall(),tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"precision = model_.evaluate(x_test_data,\n                            y_test,\n                            batch_size = 32)\nresultados.append(precision)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's avaliate our networks in some conditions","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\ndef custom_f1(y_true, y_pred):    \n    def recall_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        \n        recall = TP / (Positives+K.epsilon())    \n        return recall \n    \n    \n    def precision_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    \n        precision = TP / (Pred_Positives+K.epsilon())\n        return precision \n    \n    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n    \n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef NEX(np_pixel_array, percentage):\n  percentage_zeros = percentage\n  propor = percentage_zeros \n  siz = np_pixel_array.shape[0] - (np_pixel_array.shape[0]*propor)\n\n\n  im = np_pixel_array\n  FFT_img = np.fft.fft2(im)\n  FFT_img_shift = np.fft.fftshift(FFT_img)\n\n  real = FFT_img_shift.real\n  phases = FFT_img_shift.imag\n\n  real[int(siz):,:] = 0 #Ex.: P/ baixo de certo número é 0\n  real_mod = real\n\n  phases[int(siz):,:] = 0 #Ex.: P/ baixo de certo número é 0\n\n  FFT_img_abs = np.abs(FFT_img)\n  FFT_img_shift_log = 10*np.log(np.fft.fftshift(FFT_img_abs))\n  FFT_img_shift_log[int(siz):,:] = 0\n\n  FFT_img_shift_mod = np.empty(real.shape, dtype=complex)\n\n  FFT_img_shift_mod.real = real_mod\n  FFT_img_shift_mod.imag = phases\n\n  FFT_img_mod = np.fft.ifftshift(FFT_img_shift_mod)\n  img_mod = np.fft.ifft2(FFT_img_mod)\n  img_mod = np.abs(img_mod)\n  return FFT_img_shift_log, img_mod\n\ndef cut_sup_inf(np_pixel_array, percentage):\n  im = np_pixel_array\n  FFT_img = np.fft.fft2(im)\n  FFT_img_shift = np.fft.fftshift(FFT_img)\n  FFT_img_shift_zeros = np.zeros(FFT_img_shift.shape)\n\n  percentage_zeros = percentage\n  percentage_data = 1 - percentage_zeros\n  percentage_rows = int(FFT_img_shift.shape[0]*percentage_data)\n  sup_row = int((FFT_img_shift.shape[0]/2)-(percentage_rows/2))\n  inf_row = int((FFT_img_shift.shape[0]/2)+(percentage_rows/2))\n\n  for i in range(0,int(FFT_img_shift.shape[0])):\n    for j in range(0,int(FFT_img_shift.shape[1])):\n      if (i >= sup_row) and (i <= inf_row):\n        FFT_img_shift_zeros[i][j] = 1\n\n  mask = FFT_img_shift_zeros\n  FFT_img_shift_mask = FFT_img_shift*mask\n  FFT_img_shift_mask_visu = 10*np.log(np.fft.fftshift(np.abs(FFT_img)))*mask\n\n  FFT_img_inv = np.fft.ifftshift(FFT_img_shift_mask)\n  img_inv = np.fft.ifft2(FFT_img_inv)\n  img_mod = np.abs(img_inv)\n  return FFT_img_shift_mask_visu, img_mod\n\ndef low_pass(np_pixel_array, r):\n  im = np_pixel_array\n  FFT_img = np.fft.fft2(im)\n  FFT_img_shift = np.fft.fftshift(FFT_img)\n  FFT_img_shift_zeros = np.zeros(FFT_img_shift.shape)\n\n  #r = 1 # how narrower the window is (maior pior)\n  ham = np.hamming(FFT_img_shift.shape[0])[:,None] # 1D hamming\n  ham2d = np.sqrt(np.dot(ham, ham.T)) ** r # expand to 2D hamming\n\n  mask = FFT_img_shift_zeros\n  FFT_img_shift_mask = FFT_img_shift*ham2d \n  FFT_img_shift_log = 10*np.log(np.fft.fftshift(np.abs(FFT_img)))*ham2d \n\n  FFT_img_inv = np.fft.ifftshift(FFT_img_shift_mask)\n  img_inv = np.fft.ifft2(FFT_img_inv)\n  img_mod = np.abs(img_inv)\n  return FFT_img_shift_log, img_mod\n\n\ndef noise(np_pixel_array, sigma):\n  img = np_pixel_array\n  gauss = img + np.random.normal(0,sigma,img.shape)\n  img_gauss = np.clip(gauss, 0, 255)\n  return img_gauss","metadata":{"execution":{"iopub.status.busy":"2022-02-24T02:16:16.339739Z","iopub.execute_input":"2022-02-24T02:16:16.340163Z","iopub.status.idle":"2022-02-24T02:16:16.363231Z","shell.execute_reply.started":"2022-02-24T02:16:16.340124Z","shell.execute_reply":"2022-02-24T02:16:16.362262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados = []\n\nn = 1 #1, 1,100 ,10\njump = 0.1 # 0.1, 0.1, 10, 1\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)\n\nfor i in tqdm(range(0,10,1)):\n    if jump == 0.1:\n        j = i/10\n    if jump == 10:\n        j = i*10\n    if jump == 1:\n        j = i\n    x_test_data = []\n    for number in range(0,613):\n        im = cv2.imread('../input/images-grayscale/X_test_png/image0' + str(number) + '.png',cv2.IMREAD_GRAYSCALE)\n        im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n        _, im = NEX(im,j) # mask percentage\n        #_, im = cut_sup_inf(im, j) # mask percentage\n        #_, im = low_pass(im, j) #size of window\n        #im = noise(im, j) # sigma\n        x_test_data.append(im)\n    x_test_data = (np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1))).astype('float16') / 255\n    dependencies = {'custom_f1': custom_f1}\n    model_= tf.keras.models.load_model('../input/classification-networks/MRI_mobilenet_class.h5', custom_objects=dependencies)\n    model_.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[tf.keras.metrics.AUC(multi_label=True), custom_f1,'accuracy',\n                                                                              tf.keras.metrics.Precision(), keras.metrics.Recall(),tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n    precision = model_.evaluate(x_test_data,y_test,batch_size = 32)\n    resultados.append(precision)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados = np.array(resultados)\nmatrix={}\nmatrix['loss']= resultados[:,0]\nmatrix['AUC']= resultados[:,1]\nmatrix['F1']= resultados[:,2]\nmatrix['Acc']= resultados[:,3]\nmatrix['Prec']= resultados[:,4]\nmatrix['Rec']= resultados[:,5]\nresults = pd.DataFrame(matrix)\nresults","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.to_excel(\"NEX_Mobilenet_class.xlsx\")  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------------------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"resultados = []\n\nn = 1 #1, 1,100 ,10\njump = 0.1 # 0.1, 0.1, 10, 1\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)\n\nfor i in tqdm(range(0,10,1)):\n    if jump == 0.1:\n        j = i/10\n    if jump == 10:\n        j = i*10\n    if jump == 1:\n        j = i\n    x_test_data = []\n    for number in range(0,613):\n        im = cv2.imread('../input/images-grayscale/X_test_png/image0' + str(number) + '.png',cv2.IMREAD_GRAYSCALE)\n        im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n        #_, im = NEX(im,j) # mask percentage\n        _, im = cut_sup_inf(im, j) # mask percentage\n        #_, im = low_pass(im, j) #size of window\n        #im = noise(im, j) # sigma\n        x_test_data.append(im)\n    x_test_data = (np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1))).astype('float16') / 255\n    dependencies = {'custom_f1': custom_f1}\n    model_= tf.keras.models.load_model('../input/classification-networks/MRI_mobilenet_class.h5', custom_objects=dependencies)\n    model_.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[tf.keras.metrics.AUC(multi_label=True), custom_f1,'accuracy',\n                                                                              tf.keras.metrics.Precision(), keras.metrics.Recall(),tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n    precision = model_.evaluate(x_test_data,y_test,batch_size = 32)\n    resultados.append(precision)\n\nresultados = np.array(resultados)\nmatrix={}\nmatrix['loss']= resultados[:,0]\nmatrix['AUC']= resultados[:,1]\nmatrix['F1']= resultados[:,2]\nmatrix['Acc']= resultados[:,3]\nmatrix['Prec']= resultados[:,4]\nmatrix['Rec']= resultados[:,5]\nresults = pd.DataFrame(matrix)\n\nresults.to_excel(\"CUT_SUP_INF_Mobilenet_class.xlsx\")  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"-----------------------------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"resultados = []\n\nn = 100 #1, 1,100 ,10\njump = 10 # 0.1, 0.1, 10, 1\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)\n\nfor i in tqdm(range(0,10,1)):\n    if jump == 0.1:\n        j = i/10\n    if jump == 10:\n        j = i*10\n    if jump == 1:\n        j = i\n    x_test_data = []\n    for number in range(0,613):\n        im = cv2.imread('../input/images-grayscale/X_test_png/image0' + str(number) + '.png',cv2.IMREAD_GRAYSCALE)\n        im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n        #_, im = NEX(im,j) # mask percentage\n        #_, im = cut_sup_inf(im, j) # mask percentage\n        _, im = low_pass(im, j) #size of window\n        #im = noise(im, j) # sigma\n        x_test_data.append(im)\n    x_test_data = (np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1))).astype('float16') / 255\n    dependencies = {'custom_f1': custom_f1}\n    model_= tf.keras.models.load_model('../input/classification-networks/MRI_mobilenet_class.h5', custom_objects=dependencies)\n    model_.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[tf.keras.metrics.AUC(multi_label=True), custom_f1,'accuracy',\n                                                                              tf.keras.metrics.Precision(), keras.metrics.Recall(),tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n    precision = model_.evaluate(x_test_data,y_test,batch_size = 32)\n    resultados.append(precision)\n\nresultados = np.array(resultados)\nmatrix={}\nmatrix['loss']= resultados[:,0]\nmatrix['AUC']= resultados[:,1]\nmatrix['F1']= resultados[:,2]\nmatrix['Acc']= resultados[:,3]\nmatrix['Prec']= resultados[:,4]\nmatrix['Rec']= resultados[:,5]\nresults = pd.DataFrame(matrix)\n\nresults.to_excel(\"LOWPASS_Mobilenet_class.xlsx\")  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"----------------------------------------------------------------------------------------------------","metadata":{}},{"cell_type":"code","source":"resultados = []\n\nn = 100 #1, 1,100 ,100\njump = 10 # 0.1, 0.1, 10, 10\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)\n\nfor i in tqdm(range(0,10,1)):\n    if jump == 0.1:\n        j = i/10\n    if jump == 10:\n        j = i*10\n    if jump == 1:\n        j = i\n    x_test_data = []\n    for number in range(0,613):\n        im = cv2.imread('../input/images-grayscale/X_test_png/image0' + str(number) + '.png',cv2.IMREAD_GRAYSCALE)\n        im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n        #_, im = NEX(im,j) # mask percentage\n        #_, im = cut_sup_inf(im, j) # mask percentage\n        #_, im = low_pass(im, j) #size of window\n        im = noise(im, j) # sigma\n        x_test_data.append(im)\n    x_test_data = (np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1))).astype('float16') / 255\n    dependencies = {'custom_f1': custom_f1}\n    model_= tf.keras.models.load_model('../input/classification-networks/MRI_mobilenet_class.h5', custom_objects=dependencies)\n    model_.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=[tf.keras.metrics.AUC(multi_label=True), custom_f1,'accuracy',\n                                                                              tf.keras.metrics.Precision(), keras.metrics.Recall(),tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n    precision = model_.evaluate(x_test_data,y_test,batch_size = 32)\n    resultados.append(precision)\n\nresultados = np.array(resultados)\nmatrix={}\nmatrix['loss']= resultados[:,0]\nmatrix['AUC']= resultados[:,1]\nmatrix['F1']= resultados[:,2]\nmatrix['Acc']= resultados[:,3]\nmatrix['Prec']= resultados[:,4]\nmatrix['Rec']= resultados[:,5]\nresults = pd.DataFrame(matrix)\n\nresults.to_excel(\"NOISE_Mobilenet_class.xlsx\")  ","metadata":{"execution":{"iopub.status.busy":"2022-02-24T02:34:43.022617Z","iopub.execute_input":"2022-02-24T02:34:43.022903Z","iopub.status.idle":"2022-02-24T02:36:36.57088Z","shell.execute_reply.started":"2022-02-24T02:34:43.022873Z","shell.execute_reply":"2022-02-24T02:36:36.570057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Trying classify a image","metadata":{}},{"cell_type":"code","source":"img = cv2.imread('../input/images-grayscale/X_test_png/image0227.png',cv2.IMREAD_GRAYSCALE)\n#img = cv2.equalizeHist(img)\nimg = cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA)\nimg = (np.array(img).reshape((1, 256, 256, 1))).astype('float16') / 255\ndependencies = {'custom_f1': custom_f1}\nmodel_= tf.keras.models.load_model('../input/classification-networks/MRI_seresnext50_class.h5', custom_objects=dependencies)\nprediction = model_.predict(img)\nprint(prediction)\n\nif (prediction[0][0] >  prediction[0][1]) and (prediction[0][0] >  prediction[0][2]):\n    print(\"Meningioma\")\nif (prediction[0][1] >  prediction[0][0]) and (prediction[0][1] >  prediction[0][2]):\n    print(\"Glioma\")\nif (prediction[0][2] >  prediction[0][0]) and (prediction[0][2] >  prediction[0][1]):\n    print(\"Pituitary tumor\")","metadata":{"execution":{"iopub.status.busy":"2022-02-23T01:29:16.631411Z","iopub.execute_input":"2022-02-23T01:29:16.631693Z","iopub.status.idle":"2022-02-23T01:29:34.104693Z","shell.execute_reply.started":"2022-02-23T01:29:16.631664Z","shell.execute_reply":"2022-02-23T01:29:34.103937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figsize=(8, 6)\nu = cv2.imread('../input/teste2/IMG-0003-00075.png',cv2.IMREAD_GRAYSCALE)\nplt.imshow(u, cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Initializing brain tumor segmentation","metadata":{}},{"cell_type":"code","source":"# Loading the saved data for classification - PNG method\n\n#os.makedirs(\"./X_train_validation_png\")\n#os.makedirs(\"./X_test_png\")\n#os.makedirs(\"./y_train_validation_png\")\n#os.makedirs(\"./y_test_png\")\n\n\nX_train_validation_jpg = np.load('./X_train_validation.npy', allow_pickle = True)\nfor i in tqdm(range(0,2451)):    \n    X1 = cv2.convertScaleAbs(X_train_validation_jpg[i], alpha = 0.03).tolist()\n    plt.imsave('./X_train_validation_png/image0' + str(i) + '.png', X1, cmap='gray')\n    \n\nX_test_jpg = np.load('./X_test.npy', allow_pickle = True)\nfor i in tqdm(range(0,613)):    \n    X2 = cv2.convertScaleAbs(X_test_jpg[i], alpha = 0.03).tolist()\n    plt.imsave('./X_test_png/image0' + str(i) + '.png', X2, cmap='gray')\n    \ny_train_validation_jpg = np.load('./y_train_validation.npy', allow_pickle = True)\nfor i in tqdm(range(0,2451)):    \n    X1 = cv2.convertScaleAbs(y_train_validation_jpg[i], alpha = 1).tolist()\n    plt.imsave('./y_train_validation_png/image0' + str(i) + '.png', X1, cmap='gray')\n    \n\ny_test_jpg = np.load('./y_test.npy', allow_pickle = True)\nfor i in tqdm(range(0,613)):    \n    X2 = cv2.convertScaleAbs(y_test_jpg[i], alpha = 1).tolist()\n    plt.imsave('./y_test_png/image0' + str(i) + '.png', X2, cmap='gray')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save zipfile of X_train_validation\n\nimport os\nimport zipfile\n \nfantasy_zip = zipfile.ZipFile('./X_train_validation_png.zip', 'w')\n \nfor folder, subfolders, files in os.walk('./X_train_validation_png'):\n \n    for file in files:\n        if file.endswith('.png'):\n            fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), './PNEUMOTHORAX'), compress_type = zipfile.ZIP_DEFLATED)\n \nfantasy_zip.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save zipfile of X_test\n\nimport os\nimport zipfile\n \nfantasy_zip = zipfile.ZipFile('./X_test_png.zip', 'w')\n \nfor folder, subfolders, files in os.walk('./X_test_png'):\n \n    for file in files:\n        if file.endswith('.png'):\n            fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), './PNEUMOTHORAX'), compress_type = zipfile.ZIP_DEFLATED)\n \nfantasy_zip.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save zipfile of y_train_validation\n\nimport os\nimport zipfile\n \nfantasy_zip = zipfile.ZipFile('./y_train_validation_png.zip', 'w')\n \nfor folder, subfolders, files in os.walk('./y_train_validation_png'):\n \n    for file in files:\n        if file.endswith('.png'):\n            fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), './PNEUMOTHORAX'), compress_type = zipfile.ZIP_DEFLATED)\n \nfantasy_zip.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save zipfile of y_test\n\nimport os\nimport zipfile\n \nfantasy_zip = zipfile.ZipFile('./y_test_png.zip', 'w')\n \nfor folder, subfolders, files in os.walk('./y_test_png'):\n \n    for file in files:\n        if file.endswith('.png'):\n            fantasy_zip.write(os.path.join(folder, file), os.path.relpath(os.path.join(folder,file), './PNEUMOTHORAX'), compress_type = zipfile.ZIP_DEFLATED)\n \nfantasy_zip.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Good point to restart","metadata":{}},{"cell_type":"code","source":"# Read image and resize using INTER_AREA\nx_train_validation_data = list()\nx_test_data = list()\ny_train_validation_data = list()\ny_test_data = list()\nX_train = list()\nY_train = list()\n\nfor number in tqdm(range(0,2451)):\n    image = cv2.imread('../input/images-masks/X_train_validation_png/image0' + str(number) + '.png',0)\n    mask = cv2.imread('../input/images-masks/y_train_validation_png/image0' + str(number) + '.png',0)\n    #X_train.append(image)\n    #Y_train.append(mask)\n    image = cv2.resize(image, (256,256), interpolation = cv2.INTER_AREA)\n    mask = cv2.resize(mask, (256,256), interpolation = cv2.INTER_AREA)\n    X_train.append(image)\n    Y_train.append(mask)\n    x_train_validation_data.append(image)\n    y_train_validation_data.append(mask)\n    \nfor number in tqdm(range(0,613)):\n    im = cv2.imread('../input/images-masks/X_test_png/image0' + str(number) + '.png',0)\n    im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n    x_test_data.append(im)\nfor number in tqdm(range(0,613)):\n    im = cv2.imread('../input/images-masks/y_test_png/image0' + str(number) + '.png',0)\n    im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n    y_test_data.append(im)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transform = A.Compose([A.Rotate(limit=3),\n#                       A.RandomScale(scale_limit=0.3),\n#                       A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.3, rotate_limit=3),\n#                       A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n#                       A.GridDistortion(num_steps=5, distort_limit=0.2),\n#                       A.ElasticTransform(p=0.2)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for number in tqdm(range(0,1049)):\n#    image = cv2.imread('../input/images-masks/X_train_validation_png/image0' + str(number) + '.png',cv2.IMREAD_COLOR)\n#    mask = cv2.imread('../input/images-masks/y_train_validation_png/image0' + str(number) + '.png',0)\n#    \n#    transformed = transform(image= image, mask = mask)\n#    transformed_image = transformed['image']\n#    transformed_mask = transformed['mask']  \n#    \n#    image = cv2.resize(transformed_image, (256,256), interpolation = cv2.INTER_AREA)\n#    mask = cv2.resize(transformed_mask, (256,256), interpolation = cv2.INTER_AREA)\n#    \n#    x_train_validation_data.append(image)\n#    y_train_validation_data.append(mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"''''   \nfor number in tqdm(range(0,2451)):\n    image = cv2.imread('../input/images-masks/X_train_validation_png/image0' + str(number) + '.png',cv2.IMREAD_COLOR)\n    mask = cv2.imread('../input/images-masks/y_train_validation_png/image0' + str(number) + '.png',0)\n    image = cv2.resize(transformed_image, (256,256), interpolation = cv2.INTER_AREA)\n    mask = cv2.resize(transformed_mask, (256,256), interpolation = cv2.INTER_AREA)\n    x_train_validation_data.append(image)\n    y_train_validation_data.append(mask)\n''''  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Show images\n\nf, axarr = plt.subplots(2,2)\naxarr[0,0].imshow(x_train_validation_data[400], cmap='gray')\naxarr[0,1].imshow(y_train_validation_data[400], cmap='gray')\naxarr[1,0].imshow(x_train_validation_data[600], cmap='gray')\naxarr[1,1].imshow(y_train_validation_data[600], cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reshape the images\n\nx_train_validation_data = np.array(x_train_validation_data).reshape((len(x_train_validation_data), 256, 256, 1)).astype('float16') / 255\nx_test_data = np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1)).astype('float16') / 255\ny_train_validation_data = np.array(y_train_validation_data).reshape((len(y_train_validation_data), 256, 256, 1)).astype('float16') / 255\ny_test_data = np.array(y_test_data).reshape((len(y_test_data), 256, 256, 1)).astype('float16') / 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\"\nBACKBONE = 'vgg16'\nBATCH_SIZE = 16\nLR = 0.0001\nEPOCHS = 40\nn_classes = 1\nactivation = 'sigmoid'\nmodel = sm.Unet(BACKBONE, classes=n_classes, activation=activation)\noptim = keras.optimizers.Adam(LR)\ndice_loss = sm.losses.DiceLoss()\nfocal_loss = sm.losses.BinaryFocalLoss()\ntotal_loss = dice_loss + (1 * focal_loss)\nmetrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\nmodel.compile(optim, total_loss, metrics)\nhistory=model.fit(x_train_validation_data, y_train_validation_data, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(x_test_data, y_test_data))\n\"\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = []\nmasks = []\n\ntransform = A.Compose([A.Rotate(limit=3),\n                       A.RandomScale(scale_limit=0.3),\n                       A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.3, rotate_limit=3),\n                       A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n                       A.GridDistortion(num_steps=5, distort_limit=0.2),\n                       A.ElasticTransform(p=0.2)])\n\nfor image, mask in tqdm(zip(X_train, Y_train)):\n        transformed = transform(image= image, mask = mask)\n        transformed_image = transformed['image']\n        transformed_mask = transformed['mask']\n        image = cv2.resize(transformed_image, (256,256), interpolation = cv2.INTER_AREA)\n        mask = cv2.resize(transformed_mask, (256,256), interpolation = cv2.INTER_AREA)\n        images.append(image)\n        masks.append(mask)\n\nx_train_images = np.array(images).reshape((len(images), 256, 256, 1)).astype('float16') / 255\ny_train_masks = np.array(masks).reshape((len(masks), 256, 256, 1)).astype('float16') / 255  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deleting useless data\n\ndel X_train\ndel Y_train\ndel images\ndel masks\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constructing the NN and executing the tran and validation\nstart_time = datetime.datetime.now()\n\nkfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n\nresultados = []\nj = 0\n\nfor train_index, test_index in kfold.split(x_train_validation_data,np.zeros(shape = (x_train_validation_data.shape[0],1))):\n    \n    SIZE = len(x_train_validation_data[0])          \n    \n    x_train_images_ = np.append(x_train_validation_data[train_index],x_train_images[train_index][:len(x_train_images[train_index])//2], axis = 0)\n    y_train_masks_ = np.append(y_train_validation_data[train_index],y_train_masks[train_index][:len(y_train_masks[train_index])//2], axis = 0)\n    \n    with tpu_strategy.scope():\n        BACKBONE = 'seresnext50'\n        BATCH_SIZE = 8 * tpu_strategy.num_replicas_in_sync\n        LR = 0.0001\n        EPOCHS = 100\n        n_classes = 1\n        activation = 'sigmoid'\n        base_model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)\n\n        inp = keras.layers.Input(shape=(None, None, 1))\n        l1 = keras.layers.Conv2D(3, (1, 1))(inp)\n        out = base_model(l1)\n        model = keras.models.Model(inp, out, name=base_model.name)\n\n        optim = keras.optimizers.Adam(learning_rate=LR)\n        dice_loss = sm.losses.DiceLoss()\n        #focal_loss = sm.losses.BinaryFocalLoss()\n        #total_loss = dice_loss + (1 * focal_loss)\n        metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), \"accuracy\"]\n        model.compile(optim, dice_loss, metrics)\n        model.summary()\n\n        if j == 0:\n            k = 'val_iou_score'\n        else:\n            k = 'val_iou_score'#_'+str(j)\n\n        name= 'MRI_seresnext50.h5'\n        rlr = ReduceLROnPlateau(monitor = k, factor = 0.1, patience = 6, verbose = 1, min_delta = 1e-4, min_lr = 1e-5, mode = 'max', cooldown=1)\n        ckp = ModelCheckpoint(name,monitor = k,verbose = 1, save_best_only = True, mode = 'max')    \n        es = EarlyStopping(monitor = k, min_delta = 1e-4, patience =12, mode = 'max', restore_best_weights = True, verbose = 1)\n        history = model.fit(x_train_images_, y_train_masks_,\n                            batch_size = BATCH_SIZE, epochs = EPOCHS, \n                            validation_data = (x_train_validation_data[test_index], y_train_validation_data[test_index]),\n                            callbacks=[rlr, es,ckp])\n\n        dependencies = {'dice_loss': dice_loss,'iou_score': sm.metrics.IOUScore(threshold=0.5), 'f1-score': sm.metrics.FScore(threshold=0.5)}\n        model_= tf.keras.models.load_model('./MRI_seresnext50.h5', custom_objects=dependencies)\n        model_.compile(loss=dice_loss, optimizer=optim, metrics=metrics)\n        precision = model_.evaluate(x_train_validation_data[test_index],\n                                    y_train_validation_data[test_index],\n                                    batch_size = BATCH_SIZE)\n\n\n        resultados.append(precision)\n        j += 1\n        \nend_time = datetime.datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Treinar modelo**","metadata":{}},{"cell_type":"code","source":"# Read image and resize using INTER_AREA\nx_train_validation_data = list()\nx_test_data = list()\ny_train_validation_data = list()\ny_test_data = list()\nX_train = list()\nY_train = list()\n\nfor number in tqdm(range(0,2451)):\n    image = cv2.imread('../input/images-masks/X_train_validation_png/image0' + str(number) + '.png',0)\n    mask = cv2.imread('../input/images-masks/y_train_validation_png/image0' + str(number) + '.png',0)\n    image = cv2.resize(image, (256,256), interpolation = cv2.INTER_AREA)\n    mask = cv2.resize(mask, (256,256), interpolation = cv2.INTER_AREA)\n    X_train.append(image)\n    Y_train.append(mask)\n    x_train_validation_data.append(image)\n    y_train_validation_data.append(mask)\n        \nfor number in tqdm(range(0,613)):\n    im = cv2.imread('../input/images-masks/y_test_png/image0' + str(number) + '.png',0)\n    im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n    y_test_data.append(im)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reshape the images\n\nx_train_validation_data = np.array(x_train_validation_data).reshape((len(x_train_validation_data), 256, 256, 1)).astype('float16') / 255\ny_train_validation_data = np.array(y_train_validation_data).reshape((len(y_train_validation_data), 256, 256, 1)).astype('float16') / 255\ny_test_data = np.array(y_test_data).reshape((len(y_test_data), 256, 256, 1)).astype('float16') / 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = []\nmasks = []\n\ntransform = A.Compose([A.Rotate(limit=3),\n                       A.RandomScale(scale_limit=0.3),\n                       A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.3, rotate_limit=3),\n                       A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n                       A.GridDistortion(num_steps=5, distort_limit=0.2),\n                       A.ElasticTransform(p=0.2)])\n\nfor image, mask in tqdm(zip(X_train, Y_train)):\n        transformed = transform(image= image, mask = mask)\n        transformed_image = transformed['image']\n        transformed_mask = transformed['mask']\n        image = cv2.resize(transformed_image, (256,256), interpolation = cv2.INTER_AREA)\n        mask = cv2.resize(transformed_mask, (256,256), interpolation = cv2.INTER_AREA)\n        images.append(image)\n        masks.append(mask)\n\nx_train_images = np.array(images).reshape((len(images), 256, 256, 1)).astype('float16') / 255\ny_train_masks = np.array(masks).reshape((len(masks), 256, 256, 1)).astype('float16') / 255  \n\nfor number in tqdm(range(0,613)):\n    im = cv2.imread('../input/images-masks/X_test_png/image0' + str(number) + '.png',0)\n    im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n    #_, im = NEX(im,j) # mask percentage\n    #_, im = cut_sup_inf(im, j) # mask percentage\n    #_, im = low_pass(im, j) #size of window\n    #im = noise(im, j) # sigma\n    x_test_data.append(im)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test_data = np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1)).astype('float16') / 255","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Constructing the NN and executing the tran and validation\nstart_time = datetime.datetime.now()\n\nkfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\nresultados = []\n\nSIZE = len(x_train_validation_data[0])          \n\nx_train_images_ = np.append(x_train_validation_data,x_train_images[:len(x_train_images)//2], axis = 0)\ny_train_masks_ = np.append(y_train_validation_data,y_train_masks[:len(y_train_masks)//2], axis = 0)\n\n\nBACKBONE = 'mobilenet'\nBATCH_SIZE = 32\nLR = 0.0001\nEPOCHS = 100\nn_classes = 1\nactivation = 'sigmoid'\nbase_model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)\n\ninp = keras.layers.Input(shape=(None, None, 1))\nl1 = keras.layers.Conv2D(3, (1, 1))(inp)\nout = base_model(l1)\nmodel = keras.models.Model(inp, out, name=base_model.name)\n\noptim = keras.optimizers.Adam(learning_rate=LR)\ndice_loss = sm.losses.DiceLoss()\nmetrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), \"accuracy\"]\nmodel.compile(optim, dice_loss, metrics)\nmodel.summary()\n\nk = 'val_iou_score'\n\nname= 'MRI_mobilenet.h5'\nrlr = ReduceLROnPlateau(monitor = k, factor = 0.1, patience = 6, verbose = 1, min_delta = 1e-4, min_lr = 1e-5, mode = 'max', cooldown=1)\nckp = ModelCheckpoint(name,monitor = k,verbose = 1, save_best_only = True, mode = 'max')    \nes = EarlyStopping(monitor = k, min_delta = 1e-4, patience =12, mode = 'max', restore_best_weights = True, verbose = 1)\nhistory = model.fit(x_train_images_, y_train_masks_,\n                    batch_size = BATCH_SIZE, epochs = EPOCHS, \n                    validation_data = (x_test_data, y_test_data),\n                    callbacks=[rlr,es,ckp])\n\n\nend_time = datetime.datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tá terminando","metadata":{}},{"cell_type":"code","source":"y_test_data = []\n\nfor number in tqdm(range(0,613)):\n    im = cv2.imread('../input/images-masks/y_test_png/image0' + str(number) + '.png',0)\n    im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n    y_test_data.append(im)\n    \ny_test_data = np.array(y_test_data).reshape((len(y_test_data), 256, 256, 1)).astype('float16') / 255","metadata":{"execution":{"iopub.status.busy":"2022-02-24T01:36:26.277683Z","iopub.execute_input":"2022-02-24T01:36:26.278372Z","iopub.status.idle":"2022-02-24T01:36:33.894241Z","shell.execute_reply.started":"2022-02-24T01:36:26.278337Z","shell.execute_reply":"2022-02-24T01:36:33.893525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import backend as K\n\ndef custom_f1(y_true, y_pred):    \n    def recall_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        \n        recall = TP / (Positives+K.epsilon())    \n        return recall \n    \n    \n    def precision_m(y_true, y_pred):\n        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    \n        precision = TP / (Pred_Positives+K.epsilon())\n        return precision \n    \n    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n    \n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ndef NEX(np_pixel_array, percentage):\n  percentage_zeros = percentage\n  propor = percentage_zeros \n  siz = np_pixel_array.shape[0] - (np_pixel_array.shape[0]*propor)\n\n\n  im = np_pixel_array\n  FFT_img = np.fft.fft2(im)\n  FFT_img_shift = np.fft.fftshift(FFT_img)\n\n  real = FFT_img_shift.real\n  phases = FFT_img_shift.imag\n\n  real[int(siz):,:] = 0 #Ex.: P/ baixo de certo número é 0\n  real_mod = real\n\n  phases[int(siz):,:] = 0 #Ex.: P/ baixo de certo número é 0\n\n  FFT_img_abs = np.abs(FFT_img)\n  FFT_img_shift_log = 10*np.log(np.fft.fftshift(FFT_img_abs))\n  FFT_img_shift_log[int(siz):,:] = 0\n\n  FFT_img_shift_mod = np.empty(real.shape, dtype=complex)\n\n  FFT_img_shift_mod.real = real_mod\n  FFT_img_shift_mod.imag = phases\n\n  FFT_img_mod = np.fft.ifftshift(FFT_img_shift_mod)\n  img_mod = np.fft.ifft2(FFT_img_mod)\n  img_mod = np.abs(img_mod)\n  return FFT_img_shift_log, img_mod\n\ndef cut_sup_inf(np_pixel_array, percentage):\n  im = np_pixel_array\n  FFT_img = np.fft.fft2(im)\n  FFT_img_shift = np.fft.fftshift(FFT_img)\n  FFT_img_shift_zeros = np.zeros(FFT_img_shift.shape)\n\n  percentage_zeros = percentage\n  percentage_data = 1 - percentage_zeros\n  percentage_rows = int(FFT_img_shift.shape[0]*percentage_data)\n  sup_row = int((FFT_img_shift.shape[0]/2)-(percentage_rows/2))\n  inf_row = int((FFT_img_shift.shape[0]/2)+(percentage_rows/2))\n\n  for i in range(0,int(FFT_img_shift.shape[0])):\n    for j in range(0,int(FFT_img_shift.shape[1])):\n      if (i >= sup_row) and (i <= inf_row):\n        FFT_img_shift_zeros[i][j] = 1\n\n  mask = FFT_img_shift_zeros\n  FFT_img_shift_mask = FFT_img_shift*mask\n  FFT_img_shift_mask_visu = 10*np.log(np.fft.fftshift(np.abs(FFT_img)))*mask\n\n  FFT_img_inv = np.fft.ifftshift(FFT_img_shift_mask)\n  img_inv = np.fft.ifft2(FFT_img_inv)\n  img_mod = np.abs(img_inv)\n  return FFT_img_shift_mask_visu, img_mod\n\ndef low_pass(np_pixel_array, r):\n  im = np_pixel_array\n  FFT_img = np.fft.fft2(im)\n  FFT_img_shift = np.fft.fftshift(FFT_img)\n  FFT_img_shift_zeros = np.zeros(FFT_img_shift.shape)\n\n  #r = 1 # how narrower the window is (maior pior)\n  ham = np.hamming(FFT_img_shift.shape[0])[:,None] # 1D hamming\n  ham2d = np.sqrt(np.dot(ham, ham.T)) ** r # expand to 2D hamming\n\n  mask = FFT_img_shift_zeros\n  FFT_img_shift_mask = FFT_img_shift*ham2d \n  FFT_img_shift_log = 10*np.log(np.fft.fftshift(np.abs(FFT_img)))*ham2d \n\n  FFT_img_inv = np.fft.ifftshift(FFT_img_shift_mask)\n  img_inv = np.fft.ifft2(FFT_img_inv)\n  img_mod = np.abs(img_inv)\n  return FFT_img_shift_log, img_mod\n\n\ndef noise(np_pixel_array, sigma):\n  img = np_pixel_array\n  gauss = img + np.random.normal(0,sigma,img.shape)\n  img_gauss = np.clip(gauss, 0, 255)\n  return img_gauss\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T01:39:46.334188Z","iopub.execute_input":"2022-02-24T01:39:46.334516Z","iopub.status.idle":"2022-02-24T01:39:46.358253Z","shell.execute_reply.started":"2022-02-24T01:39:46.334482Z","shell.execute_reply":"2022-02-24T01:39:46.357434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados = []\nn = 1 #1, 1,100 ,10\njump = 0.1 # 0.1, 0.1, 10, 1\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)\n\nfor i in tqdm(range(0,10,1)):\n    if jump == 0.1:\n        j = i/10\n    if jump == 10:\n        j = i*10\n    if jump == 1:\n        j = i\n    x_test_data = []\n    \n    for number in tqdm(range(0,613)):\n        im = cv2.imread('../input/images-masks/X_test_png/image0' + str(number) + '.png',0)\n        im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n        _, im = NEX(im,j) # mask percentage\n        #_, im = cut_sup_inf(im, j) # mask percentage\n        #_, im = low_pass(im, j) #size of window\n        #im = noise(im, j) # sigma\n        x_test_data.append(im)\n        \n    BATCH_SIZE = 32\n    LR = 0.0001\n    EPOCHS = 100\n    optim = keras.optimizers.Adam(learning_rate=LR)\n    dice_loss = sm.losses.DiceLoss()\n    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), \"accuracy\"]\n    x_test_data = np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1)).astype('float16') / 255\n    dependencies = {'dice_loss': dice_loss,'iou_score': sm.metrics.IOUScore(threshold=0.5), 'f1-score': sm.metrics.FScore(threshold=0.5)}\n    model_= tf.keras.models.load_model('../input/segmentation-networks/MRI_mobilenet(1).h5', custom_objects=dependencies)\n    model_.compile(loss=dice_loss, optimizer=optim, metrics=metrics)\n    precision = model_.evaluate(x_test_data,\n                                y_test_data,\n                                batch_size = BATCH_SIZE)\n\n\n    resultados.append(precision)\nresultados = np.array(resultados)\nmatrix={}\nmatrix['loss']= resultados[:,0]\nmatrix['IOU']= resultados[:,1]\nmatrix['F1']= resultados[:,2]\nmatrix['Acc']= resultados[:,3]\nresults = pd.DataFrame(matrix)\n\nresults.to_excel(\"NEX_Mobilenet_seg.xlsx\")  ","metadata":{"execution":{"iopub.status.busy":"2022-02-20T03:34:03.998435Z","iopub.execute_input":"2022-02-20T03:34:03.999234Z","iopub.status.idle":"2022-02-20T03:36:47.612114Z","shell.execute_reply.started":"2022-02-20T03:34:03.999198Z","shell.execute_reply":"2022-02-20T03:36:47.611399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados = []\nn = 1 #1, 1,100 ,10\njump = 0.1 # 0.1, 0.1, 10, 1\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)\n\nfor i in tqdm(range(0,10,1)):\n    if jump == 0.1:\n        j = i/10\n    if jump == 10:\n        j = i*10\n    if jump == 1:\n        j = i\n    x_test_data = []\n    \n    for number in tqdm(range(0,613)):\n        im = cv2.imread('../input/images-masks/X_test_png/image0' + str(number) + '.png',0)\n        im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n        #_, im = NEX(im,j) # mask percentage\n        _, im = cut_sup_inf(im, j) # mask percentage\n        #_, im = low_pass(im, j) #size of window\n        #im = noise(im, j) # sigma\n        x_test_data.append(im)\n        \n    BATCH_SIZE = 32\n    LR = 0.0001\n    EPOCHS = 100    \n    optim = keras.optimizers.Adam(learning_rate=LR)\n    dice_loss = sm.losses.DiceLoss()\n    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), \"accuracy\"]\n    x_test_data = np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1)).astype('float16') / 255\n    dependencies = {'dice_loss': dice_loss,'iou_score': sm.metrics.IOUScore(threshold=0.5), 'f1-score': sm.metrics.FScore(threshold=0.5)}\n    model_= tf.keras.models.load_model('../input/segmentation-networks/MRI_mobilenet(1).h5', custom_objects=dependencies)\n    model_.compile(loss=dice_loss, optimizer=optim, metrics=metrics)\n    precision = model_.evaluate(x_test_data,\n                                y_test_data,\n                                batch_size = BATCH_SIZE)\n\n\n    resultados.append(precision)\nresultados = np.array(resultados)\nmatrix={}\nmatrix['loss']= resultados[:,0]\nmatrix['IOU']= resultados[:,1]\nmatrix['F1']= resultados[:,2]\nmatrix['Acc']= resultados[:,3]\nresults = pd.DataFrame(matrix)\n\nresults.to_excel(\"CUT_SUP_INF_Mobilenet_seg.xlsx\")  ","metadata":{"execution":{"iopub.status.busy":"2022-02-20T03:36:47.613467Z","iopub.execute_input":"2022-02-20T03:36:47.614054Z","iopub.status.idle":"2022-02-20T03:41:33.836423Z","shell.execute_reply.started":"2022-02-20T03:36:47.614018Z","shell.execute_reply":"2022-02-20T03:41:33.835279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados = []\nn = 100 #1, 1,100 ,10\njump = 10 # 0.1, 0.1, 10, 1\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)\n\nfor i in tqdm(range(0,10,1)):\n    if jump == 0.1:\n        j = i/10\n    if jump == 10:\n        j = i*10\n    if jump == 1:\n        j = i\n    x_test_data = []\n    \n    for number in tqdm(range(0,613)):\n        im = cv2.imread('../input/images-masks/X_test_png/image0' + str(number) + '.png',0)\n        im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n        #_, im = NEX(im,j) # mask percentage\n        #_, im = cut_sup_inf(im, j) # mask percentage\n        _, im = low_pass(im, j) #size of window\n        #im = noise(im, j) # sigma\n        x_test_data.append(im)\n        \n    BATCH_SIZE = 32\n    LR = 0.0001\n    EPOCHS = 100\n    optim = keras.optimizers.Adam(learning_rate=LR)\n    dice_loss = sm.losses.DiceLoss()\n    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), \"accuracy\"]\n    x_test_data = np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1)).astype('float16') / 255\n    dependencies = {'dice_loss': dice_loss,'iou_score': sm.metrics.IOUScore(threshold=0.5), 'f1-score': sm.metrics.FScore(threshold=0.5)}\n    model_= tf.keras.models.load_model('../input/segmentation-networks/MRI_mobilenet(1).h5', custom_objects=dependencies)\n    model_.compile(loss=dice_loss, optimizer=optim, metrics=metrics)\n    precision = model_.evaluate(x_test_data,\n                                y_test_data,\n                                batch_size = BATCH_SIZE)\n\n\n    resultados.append(precision)\nresultados = np.array(resultados)\nmatrix={}\nmatrix['loss']= resultados[:,0]\nmatrix['IOU']= resultados[:,1]\nmatrix['F1']= resultados[:,2]\nmatrix['Acc']= resultados[:,3]\nresults = pd.DataFrame(matrix)\n\nresults.to_excel(\"LOWPASS_Mobilenet_seg.xlsx\")  ","metadata":{"execution":{"iopub.status.busy":"2022-02-20T13:19:27.205352Z","iopub.execute_input":"2022-02-20T13:19:27.206226Z","iopub.status.idle":"2022-02-20T13:22:25.640727Z","shell.execute_reply.started":"2022-02-20T13:19:27.206189Z","shell.execute_reply":"2022-02-20T13:22:25.63976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultados = []\nn = 100 #1, 1,100 ,100\njump = 10 # 0.1, 0.1, 10, 10\ny_test = np.load('../input/brain-tumor-dataset-modified/y_test.npy', allow_pickle = True)\n\nfor i in tqdm(range(0,10,1)):\n    if jump == 0.1:\n        j = i/10\n    if jump == 10:\n        j = i*10\n    if jump == 1:\n        j = i\n    x_test_data = []\n    \n    for number in tqdm(range(0,613)):\n        im = cv2.imread('../input/images-masks/X_test_png/image0' + str(number) + '.png',0)\n        im = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n        #_, im = NEX(im,j) # mask percentage\n        #_, im = cut_sup_inf(im, j) # mask percentage\n        #_, im = low_pass(im, j) #size of window\n        im = noise(im, j) # sigma\n        x_test_data.append(im)\n\n    BATCH_SIZE = 32\n    LR = 0.0001\n    EPOCHS = 100\n    optim = keras.optimizers.Adam(learning_rate=LR)\n    dice_loss = sm.losses.DiceLoss()\n    metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), \"accuracy\"]\n    x_test_data = np.array(x_test_data).reshape((len(x_test_data), 256, 256, 1)).astype('float16') / 255\n    dependencies = {'dice_loss': dice_loss,'iou_score': sm.metrics.IOUScore(threshold=0.5), 'f1-score': sm.metrics.FScore(threshold=0.5)}\n    model_= tf.keras.models.load_model('../input/segmentation-networks/MRI_mobilenet(1).h5', custom_objects=dependencies)\n    model_.compile(loss=dice_loss, optimizer=optim, metrics=metrics)\n    precision = model_.evaluate(x_test_data,\n                                y_test_data,\n                                batch_size = BATCH_SIZE)\n\n\n    resultados.append(precision)\nresultados = np.array(resultados)\nmatrix={}\nmatrix['loss']= resultados[:,0]\nmatrix['IOU']= resultados[:,1]\nmatrix['F1']= resultados[:,2]\nmatrix['Acc']= resultados[:,3]\nresults = pd.DataFrame(matrix)\n\nresults.to_excel(\"NOISE_Mobilenet_seg.xlsx\")  ","metadata":{"execution":{"iopub.status.busy":"2022-02-24T01:58:35.896105Z","iopub.execute_input":"2022-02-24T01:58:35.896366Z","iopub.status.idle":"2022-02-24T02:00:48.975139Z","shell.execute_reply.started":"2022-02-24T01:58:35.896337Z","shell.execute_reply":"2022-02-24T02:00:48.974305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fazendo imagens para o artigo","metadata":{}},{"cell_type":"code","source":"im = cv2.imread('../input/images-masks/X_test_png/image0227.jpg',0)\nim = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im = cv2.imread('../input/caso-uh/IMG-0001-00006.png',0)\nim = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\nma = cv2.imread('../input/images-masks/y_test_png/image0227.png',0)\nma = cv2.resize(ma, (256,256), interpolation = cv2.INTER_AREA)\n\nim = np.array(im).reshape((1, 256, 256, 1)).astype('float16') #/ 255\nma = np.array(ma).reshape((1, 256, 256, 1)).astype('float16') #/ 255\n\n\nLR = 0.0001\noptim = keras.optimizers.Adam(learning_rate=LR)\ndice_loss = sm.losses.DiceLoss()\nmetrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), \"accuracy\"]\ndependencies = {'dice_loss': dice_loss,'iou_score': sm.metrics.IOUScore(threshold=0.5), 'f1-score': sm.metrics.FScore(threshold=0.5)}\nmodel_= tf.keras.models.load_model('../input/segmentation-networks/MRI_vgg16(2).h5', custom_objects=dependencies)\nmodel_.compile(loss=dice_loss, optimizer=optim, metrics=metrics)\n\npredicted = model_.predict(im.astype(np.float16))\npredicted = np.squeeze(predicted.astype(np.float16))\nplt.imsave('predicted_000.jpg', predicted, cmap='gray')\npredicted_000 = cv2.imread('./predicted_000.jpg',0)\n\nim = cv2.imread('../input/caso-uh/IMG-0001-00006.png',0)\nim = cv2.resize(im, (256,256), interpolation = cv2.INTER_AREA)\nstack = np.hstack((im,predicted_000))\ncv2.imwrite('stack.png',stack)\n\nfig = plt.subplots(figsize=(10,26))\nstack = cv2.imread('./stack.png',1)\nplt.imshow(stack)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:20:49.093439Z","iopub.execute_input":"2022-03-08T21:20:49.094280Z","iopub.status.idle":"2022-03-08T21:20:53.503311Z","shell.execute_reply.started":"2022-03-08T21:20:49.094233Z","shell.execute_reply":"2022-03-08T21:20:53.502544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.subplots(figsize=(10,10))\nplt.imshow(cv2.addWeighted( im, 0.6, predicted_000, 0.9,0.0),cmap='gray')\nplt.show()\ncv2.imwrite('sum.png',cv2.addWeighted( im, 0.6, predicted_000, 0.9,0.0))","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:18:05.050430Z","iopub.execute_input":"2022-03-08T21:18:05.050829Z","iopub.status.idle":"2022-03-08T21:18:05.390251Z","shell.execute_reply.started":"2022-03-08T21:18:05.050763Z","shell.execute_reply":"2022-03-08T21:18:05.389500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.subplots(figsize=(10,10))\nplt.imshow(predicted_000,cmap='gray')\nplt.show()\ncv2.imwrite('predicted_000.png',predicted_000)","metadata":{"execution":{"iopub.status.busy":"2022-03-08T21:18:09.302638Z","iopub.execute_input":"2022-03-08T21:18:09.303127Z","iopub.status.idle":"2022-03-08T21:18:09.557260Z","shell.execute_reply.started":"2022-03-08T21:18:09.303091Z","shell.execute_reply":"2022-03-08T21:18:09.556564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. Interesting things","metadata":{}},{"cell_type":"code","source":"# Read .mat and convert to numpy\n#Extra: data_dict.key() --> See the key\n\ndata_dict = mat73.loadmat('../input/brain-tumor-dataset-original/brainTumorDataPublic_1-766/1.mat')\nimgplot = plt.imshow(data_dict['cjdata']['image'], cmap = 'gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read .mat and convert to numpy\n#Extra: data_dict.key() --> See the key\n\ndata_dict = mat73.loadmat('../input/brain-tumor-dataset-original/brainTumorDataPublic_1-766/1.mat')\nimgplot = plt.imshow(data_dict['cjdata']['tumorMask'], cmap = 'gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read .mat and convert to numpy\n#Extra: data_dict.key() --> See the key\n\ndata_dict = mat73.loadmat('../input/brain-tumor-dataset-original/brainTumorDataPublic_1-766/1.mat')\ndata_dict['cjdata']['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://www.kaggle.com/vbookshelf/play-audio-read-the-files-create-a-spectrogram\n\nfrom pydub import AudioSegment\nimport IPython\n\npath = '../input/alarm-sound/Alarm_Slow_A1_fesliyanstudios.mp3'\n\nIPython.display.Audio(path, autoplay=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}